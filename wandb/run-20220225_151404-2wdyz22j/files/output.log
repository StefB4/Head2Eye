Shape surrogate pairs: (800, 2, 201)
Shape surrogate quadrupels: (800, 4, 201)
800 total pairs
560 training pairs
120 validation pairs
120 test pairs
(560, 201)
(560, 201)
(120, 201)
(120, 201)
inputs["encoder_inputs"].shape: (64, 200)
inputs["decoder_inputs"].shape: (64, 200)
targets.shape: (64, 200)
Positional Embedding Shape:
(200, 4)
Positional Embedding dense projection of inputs:
Tensor("positional_embedding/sequential/reshape/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("positional_embedding/add:0", shape=(None, 200, 4), dtype=float32)
Encoder Inputs:
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='encoder_inputs'), name='encoder_inputs', description="created by layer 'encoder_inputs'")
x_encoder:
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='positional_embedding/add_1:0', description="created by layer 'positional_embedding'")
Encoder reshaped inputs
Tensor("Placeholder:0", shape=(None, 200, 4), dtype=float32)
Encoder proj_input
Tensor("transformer_encoder/layer_normalization/add:0", shape=(None, 200, 4), dtype=float32)
Encoder proj_output
Tensor("transformer_encoder/sequential_1/dense_2/BiasAdd:0", shape=(None, 200, 4), dtype=float32)
Encoder Shape
Tensor("transformer_encoder/layer_normalization_1/add:0", shape=(None, 200, 4), dtype=float32)
encoder_outputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='transformer_encoder/layer_normalization_1/add_1:0', description="created by layer 'transformer_encoder'")
Positional Embedding Shape:
(200, 4)
Positional Embedding dense projection of inputs:
Tensor("positional_embedding_1/sequential_2/reshape_1/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("positional_embedding_1/add:0", shape=(None, 200, 4), dtype=float32)
decoder_inputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='decoder_inputs'), name='decoder_inputs', description="created by layer 'decoder_inputs'")
encoded_seq_inputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name='decoder_state_inputs'), name='decoder_state_inputs', description="created by layer 'decoder_state_inputs'")
x_decoder
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='positional_embedding_1/add_1:0', description="created by layer 'positional_embedding_1'")
Decoder Inputs:
Tensor("Placeholder:0", shape=(None, 200, 4), dtype=float32)
Decoder attention_output_1
Tensor("transformer_decoder/multi_head_attention_1/attention_output/add:0", shape=(None, 200, 4), dtype=float32)
Decoder out_1
Tensor("transformer_decoder/layer_normalization_2/add:0", shape=(None, 200, 4), dtype=float32)
Decoder encoder_outputs
Tensor("Placeholder_1:0", shape=(None, 200, 4), dtype=float32)
Decoder Shape
Tensor("transformer_decoder/layer_normalization_4/add:0", shape=(None, 200, 4), dtype=float32)
Flatten decoder_outputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name=None), name='dense_6/BiasAdd:0', description="created by layer 'dense_6'")
Positional Embedding dense projection of inputs:
Tensor("model_1/positional_embedding_1/sequential_2/reshape_1/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("model_1/positional_embedding_1/add:0", shape=(None, 200, 4), dtype=float32)
Decoder Inputs:
Tensor("model_1/positional_embedding_1/add_1:0", shape=(None, 200, 4), dtype=float32)
Decoder attention_output_1
Tensor("model_1/transformer_decoder/multi_head_attention_1/attention_output/add:0", shape=(None, 200, 4), dtype=float32)
Decoder out_1
Tensor("model_1/transformer_decoder/layer_normalization_2/add:0", shape=(None, 200, 4), dtype=float32)
Decoder encoder_outputs
Tensor("Placeholder_1:0", shape=(None, 200, 4), dtype=float32)
Decoder Shape
Tensor("model_1/transformer_decoder/layer_normalization_4/add:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Shape:
(200, 4)
Positional Embedding dense projection of inputs:
Tensor("positional_embedding_2/sequential_4/reshape_2/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("positional_embedding_2/add:0", shape=(None, 200, 4), dtype=float32)
Encoder Inputs:
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='encoder_inputs'), name='encoder_inputs', description="created by layer 'encoder_inputs'")
x_encoder:
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='positional_embedding_2/add_1:0', description="created by layer 'positional_embedding_2'")
Encoder reshaped inputs
Tensor("Placeholder:0", shape=(None, 200, 4), dtype=float32)
Encoder proj_input
Tensor("transformer_encoder_1/layer_normalization_5/add:0", shape=(None, 200, 4), dtype=float32)
Encoder proj_output
Tensor("transformer_encoder_1/sequential_5/dense_9/BiasAdd:0", shape=(None, 200, 4), dtype=float32)
Encoder Shape
Tensor("transformer_encoder_1/layer_normalization_6/add:0", shape=(None, 200, 4), dtype=float32)
encoder_outputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='transformer_encoder_1/layer_normalization_6/add_1:0', description="created by layer 'transformer_encoder_1'")
Positional Embedding Shape:
(200, 4)
Positional Embedding dense projection of inputs:
Tensor("positional_embedding_3/sequential_6/reshape_3/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("positional_embedding_3/add:0", shape=(None, 200, 4), dtype=float32)
decoder_inputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='decoder_inputs'), name='decoder_inputs', description="created by layer 'decoder_inputs'")
encoded_seq_inputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name='decoder_state_inputs'), name='decoder_state_inputs', description="created by layer 'decoder_state_inputs'")
x_decoder
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='positional_embedding_3/add_1:0', description="created by layer 'positional_embedding_3'")
Decoder Inputs:
Tensor("Placeholder:0", shape=(None, 200, 4), dtype=float32)
Decoder attention_output_1
Tensor("transformer_decoder_1/multi_head_attention_4/attention_output/add:0", shape=(None, 200, 4), dtype=float32)
Decoder out_1
Tensor("transformer_decoder_1/layer_normalization_7/add:0", shape=(None, 200, 4), dtype=float32)
Decoder encoder_outputs
Tensor("Placeholder_1:0", shape=(None, 200, 4), dtype=float32)
Decoder Shape
Tensor("transformer_decoder_1/layer_normalization_9/add:0", shape=(None, 200, 4), dtype=float32)
Flatten decoder_outputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name=None), name='dense_13/BiasAdd:0', description="created by layer 'dense_13'")
Positional Embedding dense projection of inputs:
Tensor("model_3/positional_embedding_3/sequential_6/reshape_3/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("model_3/positional_embedding_3/add:0", shape=(None, 200, 4), dtype=float32)
Decoder Inputs:
Tensor("model_3/positional_embedding_3/add_1:0", shape=(None, 200, 4), dtype=float32)
Decoder attention_output_1
Tensor("model_3/transformer_decoder_1/multi_head_attention_4/attention_output/add:0", shape=(None, 200, 4), dtype=float32)
Decoder out_1
Tensor("model_3/transformer_decoder_1/layer_normalization_7/add:0", shape=(None, 200, 4), dtype=float32)
Decoder encoder_outputs
Tensor("Placeholder_1:0", shape=(None, 200, 4), dtype=float32)
Decoder Shape
Tensor("model_3/transformer_decoder_1/layer_normalization_9/add:0", shape=(None, 200, 4), dtype=float32)
Model: "transformer"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
encoder_inputs (InputLayer)     [(None, 200)]        0
__________________________________________________________________________________________________
positional_embedding_2 (Positio (None, 200, 4)       160800      encoder_inputs[0][0]
__________________________________________________________________________________________________
decoder_inputs (InputLayer)     [(None, 200)]        0
__________________________________________________________________________________________________
transformer_encoder_1 (Transfor (None, 200, 4)       329752      positional_embedding_2[0][0]
__________________________________________________________________________________________________
model_3 (Functional)            (None, 200)          340684      decoder_inputs[0][0]
                                                                 transformer_encoder_1[0][0]
==================================================================================================
Total params: 831,236
Trainable params: 831,236
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/4
Positional Embedding dense projection of inputs:
Tensor("transformer/positional_embedding_2/sequential_4/reshape_2/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("transformer/positional_embedding_2/add:0", shape=(None, 200, 4), dtype=float32)
Encoder reshaped inputs
Tensor("transformer/positional_embedding_2/add_1:0", shape=(None, 200, 4), dtype=float32)
Encoder proj_input
Tensor("transformer/transformer_encoder_1/layer_normalization_5/add:0", shape=(None, 200, 4), dtype=float32)
Encoder proj_output
Tensor("transformer/transformer_encoder_1/sequential_5/dense_9/BiasAdd:0", shape=(None, 200, 4), dtype=float32)
Encoder Shape
Tensor("transformer/transformer_encoder_1/layer_normalization_6/add:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding dense projection of inputs:
Tensor("transformer/model_3/positional_embedding_3/sequential_6/reshape_3/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("transformer/model_3/positional_embedding_3/add:0", shape=(None, 200, 4), dtype=float32)
Decoder Inputs:
Tensor("transformer/model_3/positional_embedding_3/add_1:0", shape=(None, 200, 4), dtype=float32)
Decoder attention_output_1
Tensor("transformer/model_3/transformer_decoder_1/multi_head_attention_4/attention_output/add:0", shape=(None, 200, 4), dtype=float32)
Decoder out_1
Tensor("transformer/model_3/transformer_decoder_1/layer_normalization_7/add:0", shape=(None, 200, 4), dtype=float32)
Decoder encoder_outputs
Tensor("transformer/transformer_encoder_1/layer_normalization_6/add_1:0", shape=(None, 200, 4), dtype=float32)
Decoder Shape
Tensor("transformer/model_3/transformer_decoder_1/layer_normalization_9/add:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding dense projection of inputs:
Tensor("transformer/positional_embedding_2/sequential_4/reshape_2/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("transformer/positional_embedding_2/add:0", shape=(None, 200, 4), dtype=float32)
Encoder reshaped inputs
Tensor("transformer/positional_embedding_2/add_1:0", shape=(None, 200, 4), dtype=float32)
Encoder proj_input
Tensor("transformer/transformer_encoder_1/layer_normalization_5/add:0", shape=(None, 200, 4), dtype=float32)
Encoder proj_output
Tensor("transformer/transformer_encoder_1/sequential_5/dense_9/BiasAdd:0", shape=(None, 200, 4), dtype=float32)
Encoder Shape
Tensor("transformer/transformer_encoder_1/layer_normalization_6/add:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding dense projection of inputs:
Tensor("transformer/model_3/positional_embedding_3/sequential_6/reshape_3/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("transformer/model_3/positional_embedding_3/add:0", shape=(None, 200, 4), dtype=float32)
Decoder Inputs:
Tensor("transformer/model_3/positional_embedding_3/add_1:0", shape=(None, 200, 4), dtype=float32)
Decoder attention_output_1
Tensor("transformer/model_3/transformer_decoder_1/multi_head_attention_4/attention_output/add:0", shape=(None, 200, 4), dtype=float32)
Decoder out_1
Tensor("transformer/model_3/transformer_decoder_1/layer_normalization_7/add:0", shape=(None, 200, 4), dtype=float32)
Decoder encoder_outputs
Tensor("transformer/transformer_encoder_1/layer_normalization_6/add_1:0", shape=(None, 200, 4), dtype=float32)
Decoder Shape
Tensor("transformer/model_3/transformer_decoder_1/layer_normalization_9/add:0", shape=(None, 200, 4), dtype=float32)



4/9 [============>.................] - ETA: 1:00 - loss: 2.8518 - mean_squared_error: 2.8518 - mean_absolute_error: 1.3346 - mean_absolute_percentage_error: 203.4556 - cosine_proximity: 0.1795
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/Users/stefan/opt/anaconda3/envs/Body2Eye/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/Users/stefan/opt/anaconda3/envs/Body2Eye/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/Users/stefan/opt/anaconda3/envs/Body2Eye/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 152, in check_network_status
    status_response = self._interface.communicate_network_status()
  File "/Users/stefan/opt/anaconda3/envs/Body2Eye/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 125, in communicate_network_status
    resp = self._communicate_network_status(status)
  File "/Users/stefan/opt/anaconda3/envs/Body2Eye/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 388, in _communicate_network_status
    resp = self._communicate(req, local=True)
  File "/Users/stefan/opt/anaconda3/envs/Body2Eye/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 213, in _communicate
    return self._communicate_async(rec, local=local).get(timeout=timeout)
  File "/Users/stefan/opt/anaconda3/envs/Body2Eye/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 218, in _communicate_async
    raise Exception("The wandb backend process has shutdown")
Exception: The wandb backend process has shutdown
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/Users/stefan/opt/anaconda3/envs/Body2Eye/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/Users/stefan/opt/anaconda3/envs/Body2Eye/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/Users/stefan/opt/anaconda3/envs/Body2Eye/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 170, in check_status
    status_response = self._interface.communicate_stop_status()
  File "/Users/stefan/opt/anaconda3/envs/Body2Eye/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 114, in communicate_stop_status
    resp = self._communicate_stop_status(status)
  File "/Users/stefan/opt/anaconda3/envs/Body2Eye/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 378, in _communicate_stop_status
    resp = self._communicate(req, local=True)
  File "/Users/stefan/opt/anaconda3/envs/Body2Eye/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 213, in _communicate
    return self._communicate_async(rec, local=local).get(timeout=timeout)
  File "/Users/stefan/opt/anaconda3/envs/Body2Eye/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 218, in _communicate_async
    raise Exception("The wandb backend process has shutdown")
4/9 [============>.................] - ETA: 1:00 - loss: 2.8518 - mean_squared_error: 2.8518 - mean_absolute_error: 1.3346 - mean_absolute_percentage_error: 203.4556 - cosine_proximity: 0.1795Error in callback <function _WandbInit._resume_backend at 0x7fb99bb8b700> (for pre_run_cell):
Error in callback <function _WandbInit._pause_backend at 0x7fb99c33a310> (for post_run_cell):
Error in callback <function _WandbInit._resume_backend at 0x7fb99bb8b700> (for pre_run_cell):
Positional Embedding Shape:
(200, 4)
Positional Embedding dense projection of inputs:
Tensor("positional_embedding_4/sequential_8/reshape_4/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("positional_embedding_4/add:0", shape=(None, 200, 4), dtype=float32)
Encoder Inputs:
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='encoder_inputs'), name='encoder_inputs', description="created by layer 'encoder_inputs'")
x_encoder:
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='positional_embedding_4/add_1:0', description="created by layer 'positional_embedding_4'")
Tensor("Placeholder_1:0", shape=(None, 200), dtype=bool)
Tensor("transformer_encoder_2/Cast:0", shape=(None, 1, 1, 200), dtype=float32)
Encoder reshaped inputs
Tensor("Placeholder:0", shape=(None, 200, 4), dtype=float32)
Encoder proj_input
Tensor("transformer_encoder_2/layer_normalization_10/add:0", shape=(None, 200, 4), dtype=float32)
Encoder proj_output
Tensor("transformer_encoder_2/sequential_9/dense_16/BiasAdd:0", shape=(None, 200, 4), dtype=float32)
Encoder Shape
Tensor("transformer_encoder_2/layer_normalization_11/add:0", shape=(None, 200, 4), dtype=float32)
encoder_outputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='transformer_encoder_2/layer_normalization_11/add_1:0', description="created by layer 'transformer_encoder_2'")
Positional Embedding Shape:
(200, 4)
Positional Embedding dense projection of inputs:
Tensor("positional_embedding_5/sequential_10/reshape_5/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("positional_embedding_5/add:0", shape=(None, 200, 4), dtype=float32)
decoder_inputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='decoder_inputs'), name='decoder_inputs', description="created by layer 'decoder_inputs'")
encoded_seq_inputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name='decoder_state_inputs'), name='decoder_state_inputs', description="created by layer 'decoder_state_inputs'")
x_decoder
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='positional_embedding_5/add_1:0', description="created by layer 'positional_embedding_5'")
Decoder Inputs:
Tensor("Placeholder:0", shape=(None, 200, 4), dtype=float32)
Decoder attention_output_1
Tensor("transformer_decoder_2/multi_head_attention_7/attention_output/add:0", shape=(None, 200, 4), dtype=float32)
Decoder out_1
Tensor("transformer_decoder_2/layer_normalization_12/add:0", shape=(None, 200, 4), dtype=float32)
Decoder encoder_outputs
Tensor("Placeholder_1:0", shape=(None, 200, 4), dtype=float32)
Decoder Shape
Tensor("transformer_decoder_2/layer_normalization_14/add:0", shape=(None, 200, 4), dtype=float32)
Flatten decoder_outputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name=None), name='dense_20/BiasAdd:0', description="created by layer 'dense_20'")
Positional Embedding dense projection of inputs:
Tensor("model_5/positional_embedding_5/sequential_10/reshape_5/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("model_5/positional_embedding_5/add:0", shape=(None, 200, 4), dtype=float32)
Decoder Inputs:
Tensor("model_5/positional_embedding_5/add_1:0", shape=(None, 200, 4), dtype=float32)
Decoder attention_output_1
Tensor("model_5/transformer_decoder_2/multi_head_attention_7/attention_output/add:0", shape=(None, 200, 4), dtype=float32)
Decoder out_1
Tensor("model_5/transformer_decoder_2/layer_normalization_12/add:0", shape=(None, 200, 4), dtype=float32)
Decoder encoder_outputs
Tensor("Placeholder_1:0", shape=(None, 200, 4), dtype=float32)
Decoder Shape
Tensor("model_5/transformer_decoder_2/layer_normalization_14/add:0", shape=(None, 200, 4), dtype=float32)
Error in callback <function _WandbInit._pause_backend at 0x7fb99c33a310> (for post_run_cell):
Error in callback <function _WandbInit._resume_backend at 0x7fb99bb8b700> (for pre_run_cell):
Positional Embedding Shape:
(200, 4)
Positional Embedding dense projection of inputs:
Tensor("positional_embedding_6/sequential_12/reshape_6/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("positional_embedding_6/add:0", shape=(None, 200, 4), dtype=float32)
Encoder Inputs:
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='encoder_inputs'), name='encoder_inputs', description="created by layer 'encoder_inputs'")
x_encoder:
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='positional_embedding_6/add_1:0', description="created by layer 'positional_embedding_6'")
Tensor("Placeholder_1:0", shape=(None, 200), dtype=bool)
Tensor("transformer_encoder_3/Cast:0", shape=(None, 1, 1, 200), dtype=float32)
Encoder reshaped inputs
Tensor("Placeholder:0", shape=(None, 200, 4), dtype=float32)
Encoder proj_input
Tensor("transformer_encoder_3/layer_normalization_15/add:0", shape=(None, 200, 4), dtype=float32)
Encoder proj_output
Tensor("transformer_encoder_3/sequential_13/dense_23/BiasAdd:0", shape=(None, 200, 4), dtype=float32)
Encoder Shape
Tensor("transformer_encoder_3/layer_normalization_16/add:0", shape=(None, 200, 4), dtype=float32)
encoder_outputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='transformer_encoder_3/layer_normalization_16/add_1:0', description="created by layer 'transformer_encoder_3'")
Positional Embedding Shape:
(200, 4)
Positional Embedding dense projection of inputs:
Tensor("positional_embedding_7/sequential_14/reshape_7/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("positional_embedding_7/add:0", shape=(None, 200, 4), dtype=float32)
decoder_inputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='decoder_inputs'), name='decoder_inputs', description="created by layer 'decoder_inputs'")
encoded_seq_inputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name='decoder_state_inputs'), name='decoder_state_inputs', description="created by layer 'decoder_state_inputs'")
x_decoder
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='positional_embedding_7/add_1:0', description="created by layer 'positional_embedding_7'")
Decoder Inputs:
Tensor("Placeholder:0", shape=(None, 200, 4), dtype=float32)
Decoder attention_output_1
Tensor("transformer_decoder_3/multi_head_attention_10/attention_output/add:0", shape=(None, 200, 4), dtype=float32)
Decoder out_1
Tensor("transformer_decoder_3/layer_normalization_17/add:0", shape=(None, 200, 4), dtype=float32)
Decoder encoder_outputs
Tensor("Placeholder_1:0", shape=(None, 200, 4), dtype=float32)
Decoder Shape
Tensor("transformer_decoder_3/layer_normalization_19/add:0", shape=(None, 200, 4), dtype=float32)
Flatten decoder_outputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name=None), name='dense_27/BiasAdd:0', description="created by layer 'dense_27'")
Positional Embedding dense projection of inputs:
Tensor("model_7/positional_embedding_7/sequential_14/reshape_7/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("model_7/positional_embedding_7/add:0", shape=(None, 200, 4), dtype=float32)
Decoder Inputs:
Tensor("model_7/positional_embedding_7/add_1:0", shape=(None, 200, 4), dtype=float32)
Decoder attention_output_1
Tensor("model_7/transformer_decoder_3/multi_head_attention_10/attention_output/add:0", shape=(None, 200, 4), dtype=float32)
Decoder out_1
Tensor("model_7/transformer_decoder_3/layer_normalization_17/add:0", shape=(None, 200, 4), dtype=float32)
Decoder encoder_outputs
Tensor("Placeholder_1:0", shape=(None, 200, 4), dtype=float32)
Decoder Shape
Tensor("model_7/transformer_decoder_3/layer_normalization_19/add:0", shape=(None, 200, 4), dtype=float32)
Error in callback <function _WandbInit._pause_backend at 0x7fb99c33a310> (for post_run_cell):
Error in callback <function _WandbInit._resume_backend at 0x7fb99bb8b700> (for pre_run_cell):
Positional Embedding Shape:
(200, 4)
Positional Embedding dense projection of inputs:
Tensor("positional_embedding_8/sequential_16/reshape_8/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("positional_embedding_8/add:0", shape=(None, 200, 4), dtype=float32)
Encoder Inputs:
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='encoder_inputs'), name='encoder_inputs', description="created by layer 'encoder_inputs'")
x_encoder:
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='positional_embedding_8/add_1:0', description="created by layer 'positional_embedding_8'")
Tensor("Placeholder_1:0", shape=(None, 200), dtype=bool)
Tensor("transformer_encoder_4/Cast:0", shape=(None, 1, 1, 200), dtype=float32)
Encoder reshaped inputs
Tensor("Placeholder:0", shape=(None, 200, 4), dtype=float32)
Encoder proj_input
Tensor("transformer_encoder_4/layer_normalization_20/add:0", shape=(None, 200, 4), dtype=float32)
Encoder proj_output
Tensor("transformer_encoder_4/sequential_17/dense_30/BiasAdd:0", shape=(None, 200, 4), dtype=float32)
Encoder Shape
Tensor("transformer_encoder_4/layer_normalization_21/add:0", shape=(None, 200, 4), dtype=float32)
encoder_outputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='transformer_encoder_4/layer_normalization_21/add_1:0', description="created by layer 'transformer_encoder_4'")
Positional Embedding Shape:
(200, 4)
Positional Embedding dense projection of inputs:
Tensor("positional_embedding_9/sequential_18/reshape_9/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("positional_embedding_9/add:0", shape=(None, 200, 4), dtype=float32)
decoder_inputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='decoder_inputs'), name='decoder_inputs', description="created by layer 'decoder_inputs'")
encoded_seq_inputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name='decoder_state_inputs'), name='decoder_state_inputs', description="created by layer 'decoder_state_inputs'")
x_decoder
KerasTensor(type_spec=TensorSpec(shape=(None, 200, 4), dtype=tf.float32, name=None), name='positional_embedding_9/add_1:0', description="created by layer 'positional_embedding_9'")
Decoder Inputs:
Tensor("Placeholder:0", shape=(None, 200, 4), dtype=float32)
Decoder attention_output_1
Tensor("transformer_decoder_4/multi_head_attention_13/attention_output/add:0", shape=(None, 200, 4), dtype=float32)
Decoder out_1
Tensor("transformer_decoder_4/layer_normalization_22/add:0", shape=(None, 200, 4), dtype=float32)
Decoder encoder_outputs
Tensor("Placeholder_1:0", shape=(None, 200, 4), dtype=float32)
Decoder Shape
Tensor("transformer_decoder_4/layer_normalization_24/add:0", shape=(None, 200, 4), dtype=float32)
Flatten decoder_outputs
KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name=None), name='dense_34/BiasAdd:0', description="created by layer 'dense_34'")
Positional Embedding dense projection of inputs:
Tensor("model_9/positional_embedding_9/sequential_18/reshape_9/Reshape:0", shape=(None, 200, 4), dtype=float32)
Positional Embedding Return Value
Tensor("model_9/positional_embedding_9/add:0", shape=(None, 200, 4), dtype=float32)
Decoder Inputs:
Tensor("model_9/positional_embedding_9/add_1:0", shape=(None, 200, 4), dtype=float32)
Decoder attention_output_1
Tensor("model_9/transformer_decoder_4/multi_head_attention_13/attention_output/add:0", shape=(None, 200, 4), dtype=float32)
Decoder out_1
Tensor("model_9/transformer_decoder_4/layer_normalization_22/add:0", shape=(None, 200, 4), dtype=float32)
Decoder encoder_outputs
Tensor("Placeholder_1:0", shape=(None, 200, 4), dtype=float32)
Decoder Shape
Tensor("model_9/transformer_decoder_4/layer_normalization_24/add:0", shape=(None, 200, 4), dtype=float32)
Error in callback <function _WandbInit._pause_backend at 0x7fb99c33a310> (for post_run_cell):
Error in callback <function _WandbInit._resume_backend at 0x7fb99bb8b700> (for pre_run_cell):
Error in callback <function _WandbInit._pause_backend at 0x7fb99c33a310> (for post_run_cell):
Error in callback <function _WandbInit._resume_backend at 0x7fb99bb8b700> (for pre_run_cell):